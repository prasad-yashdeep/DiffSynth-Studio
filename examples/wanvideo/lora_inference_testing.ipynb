{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from: models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors\n",
      "    model_name: wan_video_dit model_class: WanModel\n",
      "        This model is initialized with extra kwargs: {'has_image_input': False, 'patch_size': [1, 2, 2], 'in_dim': 16, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}\n",
      "    The following models are loaded: ['wan_video_dit'].\n",
      "Loading models from: models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth\n",
      "    model_name: wan_video_text_encoder model_class: WanTextEncoder\n",
      "    The following models are loaded: ['wan_video_text_encoder'].\n",
      "Loading models from: models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth\n",
      "    model_name: wan_video_vae model_class: WanVideoVAE\n",
      "    The following models are loaded: ['wan_video_vae'].\n",
      "Loading LoRA models from file: ./safe_tensors/adapter_model.safetensors\n",
      "    Adding LoRA to wan_video_dit (models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors).\n",
      "    300 tensors are updated.\n",
      "Using wan_video_text_encoder from models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.\n",
      "Using wan_video_dit from models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors.\n",
      "Using wan_video_vae from models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.\n",
      "No wan_video_image_encoder models available.\n",
      "No wan_video_motion_controller models available.\n",
      "No wan_video_vace models available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 3/50 [00:20<05:22,  6.86s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffsynth import ModelManager, WanVideoPipeline, save_video, VideoData\n",
    "\n",
    "\n",
    "model_manager = ModelManager(torch_dtype=torch.bfloat16, device=\"cpu\")\n",
    "model_manager.load_models([\n",
    "    \"models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors\",\n",
    "    \"models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth\",\n",
    "    \"models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth\",\n",
    "],\n",
    "    torch_dtype=torch.float8_e4m3fn, # You can set `torch_dtype=torch.float8_e4m3fn` to enable FP8 quantization.\n",
    ")\n",
    "model_manager.load_lora(\"./safe_tensors/adapter_model.safetensors\", lora_alpha=1.0)\n",
    "pipe = WanVideoPipeline.from_model_manager(model_manager, device=\"cuda\")\n",
    "pipe.enable_vram_management(num_persistent_param_in_dit=None)\n",
    "\n",
    "prompt = \"A man wearing a white adidas t-shirt\"\n",
    "negative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n",
    "\n",
    "video = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=50,\n",
    "    seed=0, tiled=True,cfg_scale=6.5,\n",
    "    tea_cache_l1_thresh=0.05, # The larger this value is, the faster the speed, but the worse the visual quality.\n",
    "    tea_cache_model_id=\"Wan2.1-T2V-1.3B\", \n",
    "\n",
    ")\n",
    "save_video(video, \"video_adidas.mp4\", fps=30, quality=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from: models/Wan-AI/Wan2.1-T2V-14B/diffusion_pytorch_model.safetensors\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdiffsynth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelManager, WanVideoPipeline, save_video, VideoData\n\u001b[32m      5\u001b[39m model_manager = ModelManager(torch_dtype=torch.bfloat16, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/Wan-AI/Wan2.1-T2V-14B/diffusion_pytorch_model.safetensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/Wan-AI/Wan2.1-T2V-14B/models_t5_umt5-xxl-enc-bf16.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/Wan-AI/Wan2.1-T2V-14B/Wan2.1_VAE.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m model_manager.load_lora(\u001b[33m\"\u001b[39m\u001b[33mepoch_5.safetensors\u001b[39m\u001b[33m\"\u001b[39m, lora_alpha=\u001b[32m1.0\u001b[39m)\n\u001b[32m     12\u001b[39m pipe = WanVideoPipeline.from_model_manager(model_manager, device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/yash/AdFame/inference_pipeline/DiffSynth-Studio/diffsynth/models/model_manager.py:426\u001b[39m, in \u001b[36mModelManager.load_models\u001b[39m\u001b[34m(self, file_path_list, model_names, device, torch_dtype)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_models\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path_list, model_names=\u001b[38;5;28;01mNone\u001b[39;00m, device=\u001b[38;5;28;01mNone\u001b[39;00m, torch_dtype=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    425\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_path_list:\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/yash/AdFame/inference_pipeline/DiffSynth-Studio/diffsynth/models/model_manager.py:408\u001b[39m, in \u001b[36mModelManager.load_model\u001b[39m\u001b[34m(self, file_path, model_names, device, torch_dtype)\u001b[39m\n\u001b[32m    406\u001b[39m     state_dict = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_detector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_detector:\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel_detector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    409\u001b[39m         model_names, models = model_detector.load(\n\u001b[32m    410\u001b[39m             file_path, state_dict,\n\u001b[32m    411\u001b[39m             device=device, torch_dtype=torch_dtype,\n\u001b[32m    412\u001b[39m             allowed_model_names=model_names, model_manager=\u001b[38;5;28mself\u001b[39m\n\u001b[32m    413\u001b[39m         )\n\u001b[32m    414\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_names, models):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/yash/AdFame/inference_pipeline/DiffSynth-Studio/diffsynth/models/model_manager.py:165\u001b[39m, in \u001b[36mModelDetectorFromSingleFile.match\u001b[39m\u001b[34m(self, file_path, state_dict)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file_path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m os.path.isdir(file_path):\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m == \u001b[32m0\u001b[39m:\n\u001b[32m    166\u001b[39m     state_dict = load_state_dict(file_path)\n\u001b[32m    167\u001b[39m keys_hash_with_shape = hash_state_dict_keys(state_dict, with_shape=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffsynth import ModelManager, WanVideoPipeline, save_video, VideoData\n",
    "\n",
    "\n",
    "model_manager = ModelManager(torch_dtype=torch.bfloat16, device=\"cpu\")\n",
    "model_manager.load_models([\n",
    "    \"models/Wan-AI/Wan2.1-T2V-14B/diffusion_pytorch_model.safetensors\",\n",
    "    \"models/Wan-AI/Wan2.1-T2V-14B/models_t5_umt5-xxl-enc-bf16.pth\",\n",
    "    \"models/Wan-AI/Wan2.1-T2V-14B/Wan2.1_VAE.pth\",\n",
    "])\n",
    "model_manager.load_lora(\"./safe_tensors/epoch_5.safetensors\", lora_alpha=1.0)\n",
    "pipe = WanVideoPipeline.from_model_manager(model_manager, device=\"cuda\")\n",
    "pipe.enable_vram_management(num_persistent_param_in_dit=None)\n",
    "\n",
    "prompt = \"A man wearing a white adidas t-shirt\"\n",
    "negative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n",
    "\n",
    "video = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=50,\n",
    "    seed=0, tiled=True,cfg_scale=6.5,\n",
    "    tea_cache_l1_thresh=0.05, # The larger this value is, the faster the speed, but the worse the visual quality.\n",
    "    tea_cache_model_id=\"Wan2.1-T2V-14B\", \n",
    "\n",
    ")\n",
    "save_video(video, \"video_adidas_14B.mp4\", fps=30, quality=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff-Studio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
